{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d753f26b-ff6e-43e4-93b4-55a2423cab7a",
   "metadata": {},
   "source": [
    "**DEPRECATED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5a6cc-b8bb-49ff-8ba8-4bd711044a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path('../../data/')\n",
    "raw_dir = join(data_dir, 'raw')\n",
    "splits_dir = join(data_dir, 'splits')\n",
    "book_train_dir = join(raw_dir, 'book_train')\n",
    "train_targets_path = join(raw_dir, 'train.csv')\n",
    "\n",
    "os.makedirs(book_train_dir, exist_ok=True)\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a2639-1746-4e1e-abc9-c4ab081ea222",
   "metadata": {},
   "source": [
    "It seems like the competition test data contains the same stocks as the training data. This means we should be able to create models for each stock id instead of trying to make a general model for any stock. The following cell reserves 20 stocks as a final test set before submitting, but it may be more appropriate to do the same thing for each individual stock instead. For this cell and the rest of the notebook to work, you need to have the book_train data unzipped into the `data/raw/` directory. Once that's done the directory structure should look like this\n",
    "\n",
    "```\n",
    "|-- data\n",
    "|   |-- raw\n",
    "|   |   |-- book_train\n",
    "|   |   |   |-- stock_id=0\n",
    "|   |   |   |-- stock_id=1\n",
    "|   |   |   |-- ...\n",
    "|-- README.md\n",
    "|-- LICENSE\n",
    "|-- ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d54bd-45ef-4c9a-b0d1-d29655b70e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = os.listdir(book_train_dir)\n",
    "# Sort the directory names so we can always be sure this provides the same split for a given seed.\n",
    "stocks.sort()\n",
    "\n",
    "from random import sample\n",
    "\n",
    "train_stocks = sample(stocks, len(stocks) - 20)\n",
    "\n",
    "train_stocks_path = join(splits_dir, 'train_stocks.txt')\n",
    "\n",
    "with open(train_stocks_path, 'w+') as f:\n",
    "    f.write(''.join([train_stock + '\\n' for train_stock in train_stocks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7eb729-2702-4e5a-a8a2-523b977296af",
   "metadata": {},
   "source": [
    "For now we can just use these 5 stocks as toy data to do a bit of exploring with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03961adc-b679-4c1a-99e6-a77c12e80497",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_stocks_path, 'r') as f:\n",
    "    train_stocks_text = f.read()\n",
    "\n",
    "train_stocks_text.split(sep='\\n')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e9584-9f95-4b87-95fd-ef1284116949",
   "metadata": {},
   "source": [
    "Data comes in parquet files, which pandas has support for. It's possible to just read `data/raw/book_train` and get the data for every stock, but I advise against this as it almost crashed my computer with how much memory it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91042a50-3e87-4970-b79d-7587296624d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../data/raw/book_train/stock_id=96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659658d0-8d2e-455b-8b1d-f9ed2280b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fad4c5-dfc3-4590-b36b-9928a7085867",
   "metadata": {},
   "source": [
    "Since every row can be uniquely identified by `time_id` and `seconds_in_bucket`, reindexing based on these columns makes analysis a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa28791-4b0f-4ed0-9b0d-a0e27de45f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.MultiIndex.from_frame(df[['time_id', 'seconds_in_bucket']])\n",
    "df = df.drop(columns=['time_id', 'seconds_in_bucket'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30437b5-aa89-4d1c-a079-956c09df52fe",
   "metadata": {},
   "source": [
    "There is roughly 10 minutes of data in each time bucket. `seconds_in_bucket` is not continuous, some seconds are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c351eb-2098-4ba7-b999-e2b36d00a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8b98c-eab4-4416-8273-b03ba84ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((df['bid_price1'] + df['ask_price1']).groupby(level='time_id').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01895ef-6a69-4ee7-9c29-679c5fc33e29",
   "metadata": {},
   "source": [
    "Realized volatility calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863dc63e-61ea-486c-806a-d111bd3d3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realized_volatility(book_data):\n",
    "    wap = (book_data['bid_price1'] * book_data['ask_size1'] + book_data['ask_price1'] * book_data['bid_size1']) / (book_data['bid_size1'] + book_data['ask_size1'])\n",
    "    log_wap = np.log(wap)\n",
    "    \n",
    "    log_returns = log_wap.groupby(level='time_id').diff().dropna()\n",
    "    \n",
    "    return np.sqrt((log_returns**2).groupby(level='time_id').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d15dcb-42c5-4258-bb46-2f0852160f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_realized_volatility = realized_volatility(df)\n",
    "\n",
    "past_realized_volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018428e-53d5-493d-8faf-8a0cc4b8d359",
   "metadata": {},
   "source": [
    "We can compare this with the target realized volatility from `train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebda81d-660b-4feb-bede-8ea1374a521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets_df = pd.read_csv(train_targets_path)\n",
    "train_targets_df.index = pd.MultiIndex.from_frame(train_targets_df[['stock_id', 'time_id']])\n",
    "\n",
    "targets = train_targets_df['target']\n",
    "\n",
    "# Put both past and target in same dataframe for side-by-side comparison and graphing.\n",
    "comparison_df = pd.DataFrame({'past': past_realized_volatility, 'target': targets.loc[96]})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914b831-dc4c-4191-9858-3695b2b46a53",
   "metadata": {},
   "source": [
    "Plot of past volatility vs target volatility suggests a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827dc02-f1d4-45df-be19-286a965b6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = comparison_df.plot(x='past', y='target', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa288a-a079-4570-a502-77c252602800",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "\n",
    "For a baseline we should replicate the tutorial method of using past volatility as the predicted target. We should then try a linear model based on past volatility. From there best thing to do should be feature engineering, we still have the trade data to tap into, and there are other features we can take from the book data. For a high dimensionality approach we could try taking summary stats for short time intervals throughout each bucket and feed them into something like an SVM.\n",
    "\n",
    "A competitive model would probably recognize the data as time series data, but I want to get some practice with the more basic learning methods before getting into time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923df7b-de14-4e9b-b3c7-d4d38e9ce573",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = join(data_dir, 'processed')\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "past_realized_volatility.to_hdf(join(processed_dir, 'past_volatility_indexed.h5'), key='past_realized_volatility')\n",
    "targets.to_hdf(join(processed_dir, 'targets_indexed.h5'), 'targets')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
